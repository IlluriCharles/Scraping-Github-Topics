{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for top Repos in Github Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Information\n",
    " \n",
    " Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### project outline:\n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL \n",
    "- after the data is collected the collected data stored in data folder with title name .csv format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing  required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using request library to download the web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_topic_url(url):\n",
    "    topics_url = requests.get(url)\n",
    "    \n",
    "    if topics_url.status_code != 200:\n",
    "        raise Exception(f\"Failed to load web page {topics_url}\")\n",
    "    return topics_url.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using BeautifulSoup to  parsing and extracting information from web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_tags_info(doc):\n",
    "    selected_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_p_tags = doc.find_all('p', {'class': selected_class})\n",
    "    \n",
    "    desc_class = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc = doc.find_all('p', {'class': desc_class})\n",
    "    \n",
    "    topic_a_tag = 'no-underline flex-1 d-flex flex-column'\n",
    "    topic_link_tags = doc.find_all('a', {'class': topic_a_tag})\n",
    "    \n",
    "    return topic_p_tags, topic_desc, topic_link_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping topic information (Title, Description, URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_info(topic_p_tags, topic_desc, topic_link_tags):\n",
    "    \n",
    "    # extracting information from the tags\n",
    "    topic_titles = [tags.text for tags in topic_p_tags]\n",
    "    topic_description = [tags.text.strip() for tags in topic_desc]\n",
    "    base_url = 'https://github.com'\n",
    "    topic_url = [base_url + tags['href'] for tags in topic_link_tags]\n",
    "    \n",
    "    #storing information\n",
    "    topic_information = {\n",
    "    'Title': topic_titles,\n",
    "    'Description': topic_description,\n",
    "    'URL': topic_url\n",
    "    }\n",
    "    return topic_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating csv file with extracted information using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_csv(topic_information):\n",
    "    \n",
    "    #storing information in .csv format\n",
    "    topics_df = pd.DataFrame(topic_information)\n",
    "    topics_df.to_csv('topics.csv', index = None)\n",
    "    print(\"Created topics.csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting information out of topic page\n",
    "- open topics in github topic page\n",
    "- collect the  top repos username, repo_name, repo_url, stars in each topic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking topics pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(list_topic_url):\n",
    "    \n",
    "    # sending request to the site\n",
    "    response = requests.get(list_topic_url)\n",
    "    \n",
    "    #checking response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load web page')\n",
    "    get_topic_pages = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    return get_topic_pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting information about the tags needed to extract the information\n",
    "[![rHq2Hx.png](https://i.im.ge/2022/06/11/rHq2Hx.png)](https://im.ge/i/rHq2Hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(get_topic,i):\n",
    "    \n",
    "    #anchor tags for username, repo_url, repo_name\n",
    "    selected_h3_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    h3_tags = get_topic.find_all('h3', {'class': selected_h3_class})\n",
    "    \n",
    "    # tags to get star count\n",
    "    selected_span_tag = 'repo-stars-counter-star'\n",
    "    star_tag = get_topic.find_all('span', {'id':selected_span_tag})\n",
    "    return h3_tags, star_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting each repo information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_anchor_tags, star_tag, i):\n",
    "    \n",
    "    #collecting username, repo_name,repo_url\n",
    "    username = h3_anchor_tags[0].text.strip()\n",
    "    repo_name =h3_anchor_tags[1].text.strip()\n",
    "    base_url = 'https://github.com'\n",
    "    repo_url = base_url + h3_anchor_tags[1]['href']\n",
    "    \n",
    "    #counting total number of stars\n",
    "    star_count = star_tag[i]['title']\n",
    "    val = ''\n",
    "    for i in star_count:\n",
    "        if i != ',':\n",
    "            val +=i\n",
    "    star_count = int(val)\n",
    "    return username, repo_name, repo_url, star_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraping topics\n",
    "- scrap each topic url repo info \n",
    "- saving the information in title.csv format\n",
    "- storing each file in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_repo_info(dict_topics):\n",
    "    print(f'Scriping all topics')\n",
    "    for i in range(len(dict_topics['URL'])):\n",
    "        info = get_topic_page(dict_topics['URL'][i])\n",
    "\n",
    "        h3_tags, star_values = get_tags(info, i)\n",
    "\n",
    "        topic_dict_info = {\n",
    "            'username': [],\n",
    "            'repo_name': [],\n",
    "            'repo_url': [],\n",
    "            'star_count': []\n",
    "        }\n",
    "\n",
    "\n",
    "        for j in range(len(h3_tags)):\n",
    "            h3_anchor_tags = h3_tags[j].find_all('a')\n",
    "            topic_info = get_repo_info(h3_anchor_tags,star_values, j)\n",
    "            topic_dict_info['username'].append(topic_info[0])\n",
    "            topic_dict_info['repo_name'].append(topic_info[1])\n",
    "            topic_dict_info['repo_url'].append(topic_info[2])\n",
    "            topic_dict_info['star_count'].append(topic_info[3])\n",
    "        # print(dict_topics['Title'][i])\n",
    "        title = dict_topics['Title'][i]\n",
    "        print(f\"scraping top repo for {title}\")\n",
    "\n",
    "        # creating data directory \n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        fname = 'data/'+title + '.csv'\n",
    "\n",
    "        #checking if files are already exist\n",
    "        if os.path.exists(fname):\n",
    "            print(f'file {title}.csv already exist. skipping..')\n",
    "            continue\n",
    "\n",
    "        topic_df = pd.DataFrame(topic_dict_info)\n",
    "\n",
    "        # storing files in data folder with title.csv format\n",
    "        topic_df.to_csv('data/' + title+'.csv', index = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combining all functions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(url):\n",
    "    topic_url = scrap_topic_url(url)\n",
    "    doc = BeautifulSoup(topic_url, 'html.parser')\n",
    "    topic_tags = topic_tags_info(doc)\n",
    "    topic_information = topic_info(topic_tags[0], topic_tags[1], topic_tags[2])\n",
    "    create_topic_csv(topic_information)\n",
    "    scrap_repo_info(topic_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/topics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
